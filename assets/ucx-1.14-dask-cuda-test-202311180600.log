============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-11-18 06:37:50,878 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:37:50,883 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32959 instead
  warnings.warn(
2023-11-18 06:37:50,888 - distributed.scheduler - INFO - State start
2023-11-18 06:37:51,353 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:37:51,354 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-18 06:37:51,355 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32959/status
2023-11-18 06:37:51,356 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:37:51,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36441'
2023-11-18 06:37:51,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42461'
2023-11-18 06:37:51,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42879'
2023-11-18 06:37:51,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33027'
2023-11-18 06:37:53,026 - distributed.scheduler - INFO - Receive client connection: Client-fdb695e4-85dc-11ee-b255-d8c49764f6bb
2023-11-18 06:37:53,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43538
2023-11-18 06:37:53,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:37:53,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:37:53,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:37:53,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:37:53,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-11-18 06:37:53,395 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33903
2023-11-18 06:37:53,395 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33903
2023-11-18 06:37:53,395 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36175
2023-11-18 06:37:53,395 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-18 06:37:53,395 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:53,395 - distributed.worker - INFO -               Threads:                          4
2023-11-18 06:37:53,396 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-18 06:37:53,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qitv0sxi
2023-11-18 06:37:53,396 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d1653ec-8ecc-4b65-83af-afcf6e826d7a
2023-11-18 06:37:53,396 - distributed.worker - INFO - Starting Worker plugin PreImport-835c8315-e44c-4d9a-a519-615e542bba2b
2023-11-18 06:37:53,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54eef1e9-9866-4f27-aabb-cd6bed04faec
2023-11-18 06:37:53,397 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:53,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:37:53,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:37:53,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:37:53,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:37:53,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:37:53,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:37:53,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:37:53,496 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33903', status: init, memory: 0, processing: 0>
2023-11-18 06:37:53,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33903
2023-11-18 06:37:53,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43544
2023-11-18 06:37:53,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:37:53,499 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-18 06:37:53,499 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:53,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-18 06:37:55,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39827
2023-11-18 06:37:55,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39827
2023-11-18 06:37:55,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43009
2023-11-18 06:37:55,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-18 06:37:55,079 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,079 - distributed.worker - INFO -               Threads:                          4
2023-11-18 06:37:55,079 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-18 06:37:55,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9hhtkdoo
2023-11-18 06:37:55,080 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a94b2e9-9565-4809-9939-aad3173b603b
2023-11-18 06:37:55,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ededf6e-c6cf-491e-8cfa-cd523d0140d3
2023-11-18 06:37:55,080 - distributed.worker - INFO - Starting Worker plugin PreImport-834c5b04-7152-48bb-9d74-1a14246f2b5f
2023-11-18 06:37:55,081 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37259
2023-11-18 06:37:55,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37259
2023-11-18 06:37:55,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38831
2023-11-18 06:37:55,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-18 06:37:55,093 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,093 - distributed.worker - INFO -               Threads:                          4
2023-11-18 06:37:55,093 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-18 06:37:55,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-4hlwf4b6
2023-11-18 06:37:55,094 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b8f7c0a-9855-4136-998b-8f68ed09fab6
2023-11-18 06:37:55,094 - distributed.worker - INFO - Starting Worker plugin PreImport-a769620a-b5c0-4705-9061-6ab5cf22f34f
2023-11-18 06:37:55,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94135374-4f51-4930-b626-b538ced8438b
2023-11-18 06:37:55,094 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,096 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39447
2023-11-18 06:37:55,096 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39447
2023-11-18 06:37:55,096 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40921
2023-11-18 06:37:55,096 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-18 06:37:55,096 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,096 - distributed.worker - INFO -               Threads:                          4
2023-11-18 06:37:55,097 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-18 06:37:55,097 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-rzbb3edl
2023-11-18 06:37:55,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1515f4f-51f3-49cd-8d94-7eec713131f3
2023-11-18 06:37:55,097 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec73686e-ea17-4f41-9e7a-521a5f364f82
2023-11-18 06:37:55,098 - distributed.worker - INFO - Starting Worker plugin PreImport-299a4ce1-4509-432a-a8c0-2f59636ec6ca
2023-11-18 06:37:55,098 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39827', status: init, memory: 0, processing: 0>
2023-11-18 06:37:55,109 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39827
2023-11-18 06:37:55,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43570
2023-11-18 06:37:55,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:37:55,111 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-18 06:37:55,111 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-18 06:37:55,125 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37259', status: init, memory: 0, processing: 0>
2023-11-18 06:37:55,125 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37259
2023-11-18 06:37:55,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43586
2023-11-18 06:37:55,127 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:37:55,127 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-18 06:37:55,128 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,132 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39447', status: init, memory: 0, processing: 0>
2023-11-18 06:37:55,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-18 06:37:55,132 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39447
2023-11-18 06:37:55,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43598
2023-11-18 06:37:55,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:37:55,135 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-18 06:37:55,135 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:37:55,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-18 06:37:55,163 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-18 06:37:55,164 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-18 06:37:55,164 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-18 06:37:55,165 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-18 06:37:55,169 - distributed.scheduler - INFO - Remove client Client-fdb695e4-85dc-11ee-b255-d8c49764f6bb
2023-11-18 06:37:55,169 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43538; closing.
2023-11-18 06:37:55,170 - distributed.scheduler - INFO - Remove client Client-fdb695e4-85dc-11ee-b255-d8c49764f6bb
2023-11-18 06:37:55,170 - distributed.scheduler - INFO - Close client connection: Client-fdb695e4-85dc-11ee-b255-d8c49764f6bb
2023-11-18 06:37:55,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36441'. Reason: nanny-close
2023-11-18 06:37:55,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42461'. Reason: nanny-close
2023-11-18 06:37:55,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:37:55,172 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42879'. Reason: nanny-close
2023-11-18 06:37:55,172 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:37:55,172 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37259. Reason: nanny-close
2023-11-18 06:37:55,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33027'. Reason: nanny-close
2023-11-18 06:37:55,173 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:37:55,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39827. Reason: nanny-close
2023-11-18 06:37:55,174 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-18 06:37:55,174 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43586; closing.
2023-11-18 06:37:55,175 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289475.1750214')
2023-11-18 06:37:55,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-18 06:37:55,175 - distributed.nanny - INFO - Worker closed
2023-11-18 06:37:55,176 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43570; closing.
2023-11-18 06:37:55,176 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289475.176484')
2023-11-18 06:37:55,176 - distributed.nanny - INFO - Worker closed
2023-11-18 06:37:55,178 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33903. Reason: nanny-close
2023-11-18 06:37:55,180 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43544; closing.
2023-11-18 06:37:55,180 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-18 06:37:55,180 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33903', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289475.1804793')
2023-11-18 06:37:55,181 - distributed.nanny - INFO - Worker closed
2023-11-18 06:37:55,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:37:55,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39447. Reason: nanny-close
2023-11-18 06:37:55,189 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-18 06:37:55,189 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43598; closing.
2023-11-18 06:37:55,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289475.1895514')
2023-11-18 06:37:55,189 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:37:55,190 - distributed.nanny - INFO - Worker closed
2023-11-18 06:37:56,589 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:37:56,589 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:37:56,589 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:37:56,590 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-18 06:37:56,591 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-11-18 06:37:59,051 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:37:59,056 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44741 instead
  warnings.warn(
2023-11-18 06:37:59,060 - distributed.scheduler - INFO - State start
2023-11-18 06:37:59,138 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:37:59,139 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:37:59,139 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44741/status
2023-11-18 06:37:59,140 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:38:00,048 - distributed.scheduler - INFO - Receive client connection: Client-028cd717-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:00,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51070
2023-11-18 06:38:00,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33579'
2023-11-18 06:38:00,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37423'
2023-11-18 06:38:00,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44321'
2023-11-18 06:38:00,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38883'
2023-11-18 06:38:00,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35299'
2023-11-18 06:38:00,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41847'
2023-11-18 06:38:00,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37855'
2023-11-18 06:38:00,511 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37343'
2023-11-18 06:38:02,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,397 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,557 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,557 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,580 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:02,696 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:02,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:02,702 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:09,476 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42319
2023-11-18 06:38:09,477 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42319
2023-11-18 06:38:09,477 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41393
2023-11-18 06:38:09,477 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,477 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,477 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,477 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,477 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-two2135v
2023-11-18 06:38:09,478 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fe71477-b389-487d-8c2c-32b01ecc5c30
2023-11-18 06:38:09,559 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44359
2023-11-18 06:38:09,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44359
2023-11-18 06:38:09,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46029
2023-11-18 06:38:09,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,560 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,560 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,561 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,561 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_2bdyman
2023-11-18 06:38:09,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2580213-21d3-4293-b089-21bab5338766
2023-11-18 06:38:09,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38691
2023-11-18 06:38:09,569 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38691
2023-11-18 06:38:09,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38357
2023-11-18 06:38:09,569 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,569 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,569 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d3t2k9g9
2023-11-18 06:38:09,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93ee4267-70a7-411a-be84-f3cca4971f28
2023-11-18 06:38:09,571 - distributed.worker - INFO - Starting Worker plugin PreImport-c0f2fbcd-8d05-4875-a822-fc7ec81bff02
2023-11-18 06:38:09,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a5b773a-0210-469d-9b49-8723866eb2aa
2023-11-18 06:38:09,577 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35539
2023-11-18 06:38:09,577 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35539
2023-11-18 06:38:09,577 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38741
2023-11-18 06:38:09,578 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,578 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,578 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,578 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8v5sp_pg
2023-11-18 06:38:09,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64c4d252-e93d-4f04-b5c2-d2aec1b53c7f
2023-11-18 06:38:09,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b45dd4f8-201d-472d-9bae-b0952d966422
2023-11-18 06:38:09,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44819
2023-11-18 06:38:09,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44819
2023-11-18 06:38:09,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32837
2023-11-18 06:38:09,591 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,591 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38687
2023-11-18 06:38:09,591 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38687
2023-11-18 06:38:09,591 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,591 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35739
2023-11-18 06:38:09,591 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,591 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4wovtqqb
2023-11-18 06:38:09,591 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,591 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,591 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g29z5wtt
2023-11-18 06:38:09,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6e977db-d88d-4961-9c45-a3deee55c90a
2023-11-18 06:38:09,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-636b8e6e-c884-40a7-b6db-7d1425556746
2023-11-18 06:38:09,592 - distributed.worker - INFO - Starting Worker plugin PreImport-a9f215f8-cc71-4a19-961e-8cce6db73398
2023-11-18 06:38:09,592 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c7f6f5e-2ec5-453b-831e-92ec26b58498
2023-11-18 06:38:09,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a788d68e-b128-4c05-b778-be3ade361504
2023-11-18 06:38:09,593 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38929
2023-11-18 06:38:09,594 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38929
2023-11-18 06:38:09,594 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42075
2023-11-18 06:38:09,594 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,594 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,594 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,595 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g3ygxprs
2023-11-18 06:38:09,595 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0189d2d4-65a8-46fd-a96a-b146ff7e71f7
2023-11-18 06:38:09,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6a5ad43-69cd-4e6e-9a00-2073dabf3357
2023-11-18 06:38:09,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43155
2023-11-18 06:38:09,603 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43155
2023-11-18 06:38:09,603 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37171
2023-11-18 06:38:09,603 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,603 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,603 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:09,604 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:09,604 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zl99_qts
2023-11-18 06:38:09,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f161bc9c-4d7c-488a-8898-b0b9aa105e89
2023-11-18 06:38:09,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17ccce6d-9b7a-47b9-aa8b-1fdec22161d9
2023-11-18 06:38:09,861 - distributed.worker - INFO - Starting Worker plugin PreImport-b43cf3e2-322b-45d4-9506-e1c47b226ce7
2023-11-18 06:38:09,861 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42319', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42319
2023-11-18 06:38:09,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51166
2023-11-18 06:38:09,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,896 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,912 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,917 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26d462a0-873a-4ec1-819e-ac32f978f955
2023-11-18 06:38:09,919 - distributed.worker - INFO - Starting Worker plugin PreImport-17b7b390-a9ba-45f5-a89c-ae03b5aec8f2
2023-11-18 06:38:09,919 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,923 - distributed.worker - INFO - Starting Worker plugin PreImport-d41cdd9a-56fe-4a11-98d0-2c9cda5176a1
2023-11-18 06:38:09,923 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,939 - distributed.worker - INFO - Starting Worker plugin PreImport-42ab4455-a437-43c7-aaa1-f81deb536ad3
2023-11-18 06:38:09,940 - distributed.worker - INFO - Starting Worker plugin PreImport-6bf0e9a6-3107-46bf-8e6f-7a3d00f36fbe
2023-11-18 06:38:09,940 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,940 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95f53c6a-7af7-4e48-9f9c-e6028649abee
2023-11-18 06:38:09,941 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,950 - distributed.worker - INFO - Starting Worker plugin PreImport-59c1afba-c8e8-4d6e-8aa9-cebdc4ffbad9
2023-11-18 06:38:09,950 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,955 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38687', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38687
2023-11-18 06:38:09,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37406
2023-11-18 06:38:09,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44359', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44359
2023-11-18 06:38:09,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37422
2023-11-18 06:38:09,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,958 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,958 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38691', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38691
2023-11-18 06:38:09,959 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,959 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37416
2023-11-18 06:38:09,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,960 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35539', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,961 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35539
2023-11-18 06:38:09,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37432
2023-11-18 06:38:09,961 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,961 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,963 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,963 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,982 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38929', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,982 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38929
2023-11-18 06:38:09,982 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37446
2023-11-18 06:38:09,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,985 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,985 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,987 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,988 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43155', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,989 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43155
2023-11-18 06:38:09,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37448
2023-11-18 06:38:09,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,991 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44819', status: init, memory: 0, processing: 0>
2023-11-18 06:38:09,992 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,992 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,992 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44819
2023-11-18 06:38:09,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37452
2023-11-18 06:38:09,994 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:09,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:09,995 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:09,995 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:09,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:10,019 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,019 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,020 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,020 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,020 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,020 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,021 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,021 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:10,025 - distributed.scheduler - INFO - Remove client Client-028cd717-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:10,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51070; closing.
2023-11-18 06:38:10,026 - distributed.scheduler - INFO - Remove client Client-028cd717-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:10,026 - distributed.scheduler - INFO - Close client connection: Client-028cd717-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:10,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33579'. Reason: nanny-close
2023-11-18 06:38:10,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37423'. Reason: nanny-close
2023-11-18 06:38:10,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35539. Reason: nanny-close
2023-11-18 06:38:10,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44321'. Reason: nanny-close
2023-11-18 06:38:10,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42319. Reason: nanny-close
2023-11-18 06:38:10,031 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38883'. Reason: nanny-close
2023-11-18 06:38:10,031 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,031 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38929. Reason: nanny-close
2023-11-18 06:38:10,031 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35299'. Reason: nanny-close
2023-11-18 06:38:10,031 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37432; closing.
2023-11-18 06:38:10,032 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38691. Reason: nanny-close
2023-11-18 06:38:10,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41847'. Reason: nanny-close
2023-11-18 06:38:10,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35539', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.0321348')
2023-11-18 06:38:10,032 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,032 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44359. Reason: nanny-close
2023-11-18 06:38:10,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37855'. Reason: nanny-close
2023-11-18 06:38:10,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37343'. Reason: nanny-close
2023-11-18 06:38:10,033 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38687. Reason: nanny-close
2023-11-18 06:38:10,033 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,033 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51166; closing.
2023-11-18 06:38:10,034 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,034 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,034 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:10,034 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,034 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42319', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.034872')
2023-11-18 06:38:10,035 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43155. Reason: nanny-close
2023-11-18 06:38:10,035 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,035 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44819. Reason: nanny-close
2023-11-18 06:38:10,035 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37446; closing.
2023-11-18 06:38:10,035 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,035 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,035 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.0363066')
2023-11-18 06:38:10,036 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,036 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37416; closing.
2023-11-18 06:38:10,036 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:10,038 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.0386496')
2023-11-18 06:38:10,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37422; closing.
2023-11-18 06:38:10,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37406; closing.
2023-11-18 06:38:10,039 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:10,040 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44359', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.0404234')
2023-11-18 06:38:10,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.0409887')
2023-11-18 06:38:10,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37452; closing.
2023-11-18 06:38:10,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37448; closing.
2023-11-18 06:38:10,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44819', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.042513')
2023-11-18 06:38:10,043 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289490.0430713')
2023-11-18 06:38:10,043 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:38:10,043 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37448>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-18 06:38:10,045 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37452>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-18 06:38:11,995 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:38:11,996 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:38:11,996 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:38:11,997 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:38:11,998 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-11-18 06:38:14,412 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:14,417 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35953 instead
  warnings.warn(
2023-11-18 06:38:14,421 - distributed.scheduler - INFO - State start
2023-11-18 06:38:14,471 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:14,472 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:38:14,473 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35953/status
2023-11-18 06:38:14,473 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:38:15,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37471'
2023-11-18 06:38:15,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40483'
2023-11-18 06:38:15,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41737'
2023-11-18 06:38:15,364 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32821'
2023-11-18 06:38:15,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45653'
2023-11-18 06:38:15,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37085'
2023-11-18 06:38:15,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41531'
2023-11-18 06:38:15,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40847'
2023-11-18 06:38:15,420 - distributed.scheduler - INFO - Receive client connection: Client-0bad5652-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:15,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37618
2023-11-18 06:38:18,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,381 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:18,402 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:18,402 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:18,407 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:22,594 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44429
2023-11-18 06:38:22,595 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44429
2023-11-18 06:38:22,595 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43525
2023-11-18 06:38:22,595 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,595 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,595 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,595 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t_mni_n1
2023-11-18 06:38:22,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0c5c0e8-466e-4a68-a42c-1b3fb47cba9d
2023-11-18 06:38:22,601 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34a81768-438c-45dd-8cfd-4705f0eed5f8
2023-11-18 06:38:22,601 - distributed.worker - INFO - Starting Worker plugin PreImport-3b585617-5bcc-407f-9747-1e3bf4dc2fec
2023-11-18 06:38:22,601 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,630 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44429', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,632 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44429
2023-11-18 06:38:22,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41028
2023-11-18 06:38:22,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,634 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,634 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,792 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32807
2023-11-18 06:38:22,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32807
2023-11-18 06:38:22,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41163
2023-11-18 06:38:22,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,793 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,793 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,793 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5aicho59
2023-11-18 06:38:22,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b49cca19-9e60-40f1-abc5-94b3bfb00c5c
2023-11-18 06:38:22,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e105c595-4bc8-4bcc-a1af-9a55f1265e3f
2023-11-18 06:38:22,807 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32889
2023-11-18 06:38:22,807 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32889
2023-11-18 06:38:22,807 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45431
2023-11-18 06:38:22,808 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,808 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,808 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,808 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ymqi2u7l
2023-11-18 06:38:22,808 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b67ee96-f5af-475c-9ede-0440c12bd302
2023-11-18 06:38:22,820 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33645
2023-11-18 06:38:22,820 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33645
2023-11-18 06:38:22,821 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36647
2023-11-18 06:38:22,821 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,821 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,821 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,821 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k0mj0ohi
2023-11-18 06:38:22,821 - distributed.worker - INFO - Starting Worker plugin PreImport-67f326a8-106d-4f8c-aa85-a9269e5e852a
2023-11-18 06:38:22,821 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae502b17-6b17-451a-98bf-ce3dd9a001f9
2023-11-18 06:38:22,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-feea0c66-4c2f-4792-bc42-993be4dbe11e
2023-11-18 06:38:22,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46535
2023-11-18 06:38:22,824 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46535
2023-11-18 06:38:22,824 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37775
2023-11-18 06:38:22,824 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,824 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,824 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,824 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m5ayli1x
2023-11-18 06:38:22,825 - distributed.worker - INFO - Starting Worker plugin PreImport-b9b6bb54-b4c2-4435-905d-26379a2c2886
2023-11-18 06:38:22,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6613eeee-947e-4dc0-841c-e03c70ee9fc4
2023-11-18 06:38:22,834 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46697
2023-11-18 06:38:22,835 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46697
2023-11-18 06:38:22,835 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39285
2023-11-18 06:38:22,835 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,835 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,835 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,835 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gdzo94a9
2023-11-18 06:38:22,836 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44fc3494-de96-4307-8425-a2870abb782a
2023-11-18 06:38:22,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37947
2023-11-18 06:38:22,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37947
2023-11-18 06:38:22,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42909
2023-11-18 06:38:22,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,841 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45939
2023-11-18 06:38:22,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45939
2023-11-18 06:38:22,841 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33457
2023-11-18 06:38:22,841 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dcy56a5k
2023-11-18 06:38:22,841 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,841 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:22,842 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:22,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cb3ahmyi
2023-11-18 06:38:22,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3927a8fa-35d0-45a7-8d31-a8a6baa7309f
2023-11-18 06:38:22,842 - distributed.worker - INFO - Starting Worker plugin PreImport-02bf93e1-8f18-4c40-837c-85218018b0a0
2023-11-18 06:38:22,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fabd9c2f-4188-4b20-87a3-1e52d7cd73a0
2023-11-18 06:38:22,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a962165f-f49e-4710-9683-9d0193f226b7
2023-11-18 06:38:22,861 - distributed.worker - INFO - Starting Worker plugin PreImport-b77e32a5-4e22-4304-beb7-6372cb944d81
2023-11-18 06:38:22,862 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,864 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-faa031f2-580a-445a-b83c-a9cf1ea2a90b
2023-11-18 06:38:22,866 - distributed.worker - INFO - Starting Worker plugin PreImport-c995cf14-7da9-46c9-9eec-1d83197858f6
2023-11-18 06:38:22,867 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,871 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5223a1c6-b283-435c-9338-37f0cc1f5009
2023-11-18 06:38:22,872 - distributed.worker - INFO - Starting Worker plugin PreImport-cfea6cb6-7d47-4200-be8e-23d7840e1511
2023-11-18 06:38:22,872 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-644f8209-68b0-4e12-b8f6-9b3ee0993ce2
2023-11-18 06:38:22,876 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,877 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,912 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c638ce6-9610-4590-b738-12f28c4401d4
2023-11-18 06:38:22,912 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,912 - distributed.worker - INFO - Starting Worker plugin PreImport-f26f84be-361e-4f40-a514-bbd1012323f0
2023-11-18 06:38:22,912 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46535', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,914 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46535
2023-11-18 06:38:22,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41074
2023-11-18 06:38:22,915 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32807', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,915 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32807
2023-11-18 06:38:22,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41040
2023-11-18 06:38:22,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,916 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32889', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,917 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32889
2023-11-18 06:38:22,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41054
2023-11-18 06:38:22,917 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,917 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33645', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,919 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33645
2023-11-18 06:38:22,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41066
2023-11-18 06:38:22,920 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37947', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,920 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37947
2023-11-18 06:38:22,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41062
2023-11-18 06:38:22,920 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,920 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,923 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,923 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,923 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,923 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,933 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46697', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46697
2023-11-18 06:38:22,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41078
2023-11-18 06:38:22,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45939', status: init, memory: 0, processing: 0>
2023-11-18 06:38:22,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45939
2023-11-18 06:38:22,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41092
2023-11-18 06:38:22,955 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,955 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:22,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:22,958 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:22,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:22,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:23,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,061 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,061 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,061 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,062 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,062 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,062 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,062 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:23,066 - distributed.scheduler - INFO - Remove client Client-0bad5652-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:23,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37618; closing.
2023-11-18 06:38:23,066 - distributed.scheduler - INFO - Remove client Client-0bad5652-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:23,067 - distributed.scheduler - INFO - Close client connection: Client-0bad5652-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:23,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37471'. Reason: nanny-close
2023-11-18 06:38:23,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40483'. Reason: nanny-close
2023-11-18 06:38:23,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41737'. Reason: nanny-close
2023-11-18 06:38:23,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45939. Reason: nanny-close
2023-11-18 06:38:23,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32821'. Reason: nanny-close
2023-11-18 06:38:23,071 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37947. Reason: nanny-close
2023-11-18 06:38:23,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,071 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32807. Reason: nanny-close
2023-11-18 06:38:23,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45653'. Reason: nanny-close
2023-11-18 06:38:23,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,072 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46535. Reason: nanny-close
2023-11-18 06:38:23,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37085'. Reason: nanny-close
2023-11-18 06:38:23,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,073 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41092; closing.
2023-11-18 06:38:23,073 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32889. Reason: nanny-close
2023-11-18 06:38:23,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41531'. Reason: nanny-close
2023-11-18 06:38:23,073 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45939', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0733762')
2023-11-18 06:38:23,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,073 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33645. Reason: nanny-close
2023-11-18 06:38:23,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40847'. Reason: nanny-close
2023-11-18 06:38:23,074 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41040; closing.
2023-11-18 06:38:23,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:23,074 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46697. Reason: nanny-close
2023-11-18 06:38:23,075 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,075 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44429. Reason: nanny-close
2023-11-18 06:38:23,075 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,075 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,075 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0754158')
2023-11-18 06:38:23,075 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41062; closing.
2023-11-18 06:38:23,076 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41074; closing.
2023-11-18 06:38:23,077 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,077 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37947', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.077026')
2023-11-18 06:38:23,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46535', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0779924')
2023-11-18 06:38:23,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:23,078 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41054; closing.
2023-11-18 06:38:23,079 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0793235')
2023-11-18 06:38:23,079 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41078; closing.
2023-11-18 06:38:23,079 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41066; closing.
2023-11-18 06:38:23,080 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41028; closing.
2023-11-18 06:38:23,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0805905')
2023-11-18 06:38:23,080 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:23,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0809734')
2023-11-18 06:38:23,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289503.0813403')
2023-11-18 06:38:23,081 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:38:24,735 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:38:24,735 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:38:24,736 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:38:24,737 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:38:24,737 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-11-18 06:38:26,738 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:26,742 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42293 instead
  warnings.warn(
2023-11-18 06:38:26,745 - distributed.scheduler - INFO - State start
2023-11-18 06:38:26,817 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:26,818 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:38:26,819 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42293/status
2023-11-18 06:38:26,819 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:38:26,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42119'
2023-11-18 06:38:26,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41077'
2023-11-18 06:38:26,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44897'
2023-11-18 06:38:26,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38467'
2023-11-18 06:38:26,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45405'
2023-11-18 06:38:26,983 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37181'
2023-11-18 06:38:26,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35621'
2023-11-18 06:38:26,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42625'
2023-11-18 06:38:27,777 - distributed.scheduler - INFO - Receive client connection: Client-133257c2-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:27,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41268
2023-11-18 06:38:28,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:28,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:28,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:28,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:28,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:28,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:28,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:28,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:28,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:28,871 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:28,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:28,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:29,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:29,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:29,200 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:29,202 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:29,202 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:29,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:29,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:29,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:29,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:29,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:29,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:29,248 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:34,254 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45527
2023-11-18 06:38:34,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45527
2023-11-18 06:38:34,254 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46173
2023-11-18 06:38:34,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,255 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,255 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,255 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e4rqha5o
2023-11-18 06:38:34,255 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9da75116-9766-4b78-bddc-56d18c071de0
2023-11-18 06:38:34,255 - distributed.worker - INFO - Starting Worker plugin PreImport-7e0630f4-0407-43d4-b870-9ff3e22e87c1
2023-11-18 06:38:34,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a955900e-518c-4d97-910f-eb0774ce59e7
2023-11-18 06:38:34,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32977
2023-11-18 06:38:34,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32977
2023-11-18 06:38:34,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46457
2023-11-18 06:38:34,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,267 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,267 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,268 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,268 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-31ju10di
2023-11-18 06:38:34,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf71a20e-62ec-4d6b-bce5-390eebb625ea
2023-11-18 06:38:34,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32833
2023-11-18 06:38:34,270 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32833
2023-11-18 06:38:34,270 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43981
2023-11-18 06:38:34,270 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,270 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,270 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,270 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mw9radsn
2023-11-18 06:38:34,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ded1c8c-8e78-40da-888c-7f30bb153b6f
2023-11-18 06:38:34,281 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43761
2023-11-18 06:38:34,282 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43761
2023-11-18 06:38:34,282 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37235
2023-11-18 06:38:34,282 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,282 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,282 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,282 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hlm3sxrj
2023-11-18 06:38:34,283 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-97ce373f-b2cc-4378-8e95-97a789e9886e
2023-11-18 06:38:34,283 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be62f662-0eaf-4875-804f-d2e125b9cf8f
2023-11-18 06:38:34,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36639
2023-11-18 06:38:34,290 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36639
2023-11-18 06:38:34,290 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37387
2023-11-18 06:38:34,291 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,291 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,291 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,291 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,291 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_c8a17by
2023-11-18 06:38:34,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc2bbbe4-e3c6-4ffd-963f-d45f4c84bb77
2023-11-18 06:38:34,292 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc1e2cc9-b9f7-4014-882a-adb418257635
2023-11-18 06:38:34,293 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42767
2023-11-18 06:38:34,294 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42767
2023-11-18 06:38:34,294 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39865
2023-11-18 06:38:34,294 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,293 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36819
2023-11-18 06:38:34,294 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,294 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36819
2023-11-18 06:38:34,294 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32885
2023-11-18 06:38:34,294 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,293 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42169
2023-11-18 06:38:34,294 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,294 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,294 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42169
2023-11-18 06:38:34,294 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,294 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vrdh__ja
2023-11-18 06:38:34,294 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43871
2023-11-18 06:38:34,294 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,294 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,294 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,294 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,294 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-burrj4pe
2023-11-18 06:38:34,294 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:34,294 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:34,295 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nq785pba
2023-11-18 06:38:34,295 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21484c71-6098-41e6-9d0a-634e0fc64bc6
2023-11-18 06:38:34,295 - distributed.worker - INFO - Starting Worker plugin PreImport-b5c7ea5f-8675-4dba-931c-d32d1dd4091d
2023-11-18 06:38:34,295 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f57f3867-a0d0-456b-8d4f-9fe728c6be8a
2023-11-18 06:38:34,295 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8095e3c8-d21d-4e9e-8600-f406d67471b8
2023-11-18 06:38:34,302 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa7f2642-3db0-4a2d-bf48-66fddd4fda58
2023-11-18 06:38:34,526 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f0c1d59-25cb-49bd-857e-c717c13a03fe
2023-11-18 06:38:34,526 - distributed.worker - INFO - Starting Worker plugin PreImport-fcbff0df-0190-450d-b66f-8a212da15fbe
2023-11-18 06:38:34,526 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,537 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1679441d-9915-4a76-a769-9a79c4e1cea7
2023-11-18 06:38:34,540 - distributed.worker - INFO - Starting Worker plugin PreImport-b75881c8-d240-4e1e-8dff-c87916bd097d
2023-11-18 06:38:34,541 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,542 - distributed.worker - INFO - Starting Worker plugin PreImport-f6982b0e-9c58-4635-9b9b-c498815852fc
2023-11-18 06:38:34,542 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,546 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2dee32d1-cdb0-4dc1-80f5-f77ffc40fecb
2023-11-18 06:38:34,547 - distributed.worker - INFO - Starting Worker plugin PreImport-84a09a4d-dd09-4808-a2cd-6427b51391c9
2023-11-18 06:38:34,548 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,548 - distributed.worker - INFO - Starting Worker plugin PreImport-94bf0f62-7bfd-4fcf-a581-013d3b881cc5
2023-11-18 06:38:34,549 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1c4b1a1-2c55-440f-96cf-f5b615c106ac
2023-11-18 06:38:34,553 - distributed.worker - INFO - Starting Worker plugin PreImport-07696c88-3b5f-4868-88b7-f8cbea15e741
2023-11-18 06:38:34,553 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,554 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32977', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,556 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32977
2023-11-18 06:38:34,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48860
2023-11-18 06:38:34,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,558 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45527', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45527
2023-11-18 06:38:34,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48862
2023-11-18 06:38:34,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,569 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,571 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43761', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,571 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43761
2023-11-18 06:38:34,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48876
2023-11-18 06:38:34,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,573 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,573 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,579 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36639', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,580 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36639
2023-11-18 06:38:34,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48882
2023-11-18 06:38:34,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,582 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,582 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32833', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32833
2023-11-18 06:38:34,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48870
2023-11-18 06:38:34,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42767', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,586 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42767
2023-11-18 06:38:34,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48894
2023-11-18 06:38:34,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42169', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42169
2023-11-18 06:38:34,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48880
2023-11-18 06:38:34,589 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,591 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36819', status: init, memory: 0, processing: 0>
2023-11-18 06:38:34,597 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36819
2023-11-18 06:38:34,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48908
2023-11-18 06:38:34,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:34,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:34,607 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:34,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,615 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:34,626 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,626 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,626 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,626 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,627 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,627 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,627 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,627 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:34,634 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:34,635 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:34,638 - distributed.scheduler - INFO - Remove client Client-133257c2-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:34,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41268; closing.
2023-11-18 06:38:34,638 - distributed.scheduler - INFO - Remove client Client-133257c2-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:34,638 - distributed.scheduler - INFO - Close client connection: Client-133257c2-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:34,639 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42119'. Reason: nanny-close
2023-11-18 06:38:34,640 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41077'. Reason: nanny-close
2023-11-18 06:38:34,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32833. Reason: nanny-close
2023-11-18 06:38:34,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44897'. Reason: nanny-close
2023-11-18 06:38:34,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36639. Reason: nanny-close
2023-11-18 06:38:34,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38467'. Reason: nanny-close
2023-11-18 06:38:34,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42767. Reason: nanny-close
2023-11-18 06:38:34,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45405'. Reason: nanny-close
2023-11-18 06:38:34,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37181'. Reason: nanny-close
2023-11-18 06:38:34,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45527. Reason: nanny-close
2023-11-18 06:38:34,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35621'. Reason: nanny-close
2023-11-18 06:38:34,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48870; closing.
2023-11-18 06:38:34,644 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42625'. Reason: nanny-close
2023-11-18 06:38:34,644 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6444297')
2023-11-18 06:38:34,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43761. Reason: nanny-close
2023-11-18 06:38:34,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,644 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,645 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32977. Reason: nanny-close
2023-11-18 06:38:34,646 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48894; closing.
2023-11-18 06:38:34,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,647 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,647 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6473486')
2023-11-18 06:38:34,647 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,647 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48882; closing.
2023-11-18 06:38:34,647 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,648 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:34,649 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36819. Reason: nanny-close
2023-11-18 06:38:34,649 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,649 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42169. Reason: nanny-close
2023-11-18 06:38:34,648 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48894>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48894>: Stream is closed
2023-11-18 06:38:34,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6511362')
2023-11-18 06:38:34,651 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48862; closing.
2023-11-18 06:38:34,651 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:34,652 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6525452')
2023-11-18 06:38:34,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48876; closing.
2023-11-18 06:38:34,653 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,653 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:34,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.65409')
2023-11-18 06:38:34,654 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48860; closing.
2023-11-18 06:38:34,655 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32977', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6553798')
2023-11-18 06:38:34,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48908; closing.
2023-11-18 06:38:34,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48880; closing.
2023-11-18 06:38:34,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36819', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6565995')
2023-11-18 06:38:34,657 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289514.6569474')
2023-11-18 06:38:34,657 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:38:34,657 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48860>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-18 06:38:36,458 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:38:36,459 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:38:36,460 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:38:36,461 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:38:36,462 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-11-18 06:38:38,643 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:38,647 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33467 instead
  warnings.warn(
2023-11-18 06:38:38,650 - distributed.scheduler - INFO - State start
2023-11-18 06:38:38,971 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:38,972 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:38:38,973 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33467/status
2023-11-18 06:38:38,973 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:38:39,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40913'
2023-11-18 06:38:39,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37525'
2023-11-18 06:38:39,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45469'
2023-11-18 06:38:39,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33469'
2023-11-18 06:38:39,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42373'
2023-11-18 06:38:39,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35687'
2023-11-18 06:38:39,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45463'
2023-11-18 06:38:39,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34425'
2023-11-18 06:38:39,749 - distributed.scheduler - INFO - Receive client connection: Client-1a43e78d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:39,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49052
2023-11-18 06:38:41,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:41,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:41,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:41,474 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:44,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42477
2023-11-18 06:38:44,946 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42477
2023-11-18 06:38:44,946 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33725
2023-11-18 06:38:44,946 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:44,946 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:44,947 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:44,947 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:44,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-foi0i529
2023-11-18 06:38:44,948 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25360614-3402-4e1b-bec9-13e58c552987
2023-11-18 06:38:44,948 - distributed.worker - INFO - Starting Worker plugin PreImport-852ede36-73d0-438f-83a9-b2458c720ee9
2023-11-18 06:38:44,948 - distributed.worker - INFO - Starting Worker plugin RMMSetup-871e4dd4-822f-4da5-a86f-ff735bf4475d
2023-11-18 06:38:45,231 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42439
2023-11-18 06:38:45,232 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42439
2023-11-18 06:38:45,232 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43591
2023-11-18 06:38:45,232 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,232 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,232 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,233 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,233 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1_fu1tgy
2023-11-18 06:38:45,233 - distributed.worker - INFO - Starting Worker plugin PreImport-08c92288-b848-4066-a2fe-6ef9b23afcfc
2023-11-18 06:38:45,233 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a81d8319-cecb-4da1-b0d2-20e6f1a2b3aa
2023-11-18 06:38:45,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df6c6906-f33e-4dcd-9ec3-4fd90862a721
2023-11-18 06:38:45,236 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39577
2023-11-18 06:38:45,237 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39577
2023-11-18 06:38:45,238 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33749
2023-11-18 06:38:45,238 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,238 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,238 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,238 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k53yaou0
2023-11-18 06:38:45,239 - distributed.worker - INFO - Starting Worker plugin RMMSetup-738ec8e6-424a-4ab6-bb6a-da3d4de71001
2023-11-18 06:38:45,241 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43513
2023-11-18 06:38:45,242 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43513
2023-11-18 06:38:45,243 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37997
2023-11-18 06:38:45,243 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,243 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,243 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,243 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tz96o_cx
2023-11-18 06:38:45,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f530256-7b81-4275-807c-3ba7f78078a5
2023-11-18 06:38:45,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42197
2023-11-18 06:38:45,785 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42197
2023-11-18 06:38:45,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41667
2023-11-18 06:38:45,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,786 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,786 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,786 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p9mdbu8o
2023-11-18 06:38:45,787 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0be5f813-b1b4-4ffb-a8e2-112f23879938
2023-11-18 06:38:45,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38667
2023-11-18 06:38:45,803 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38667
2023-11-18 06:38:45,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40747
2023-11-18 06:38:45,803 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,803 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,803 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,803 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6q6x0w7f
2023-11-18 06:38:45,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b773082-2462-4ef9-9e70-35b27534fb43
2023-11-18 06:38:45,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd47f1b3-370f-4f81-8aed-bab0b4c11c1c
2023-11-18 06:38:45,809 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43833
2023-11-18 06:38:45,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43833
2023-11-18 06:38:45,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41431
2023-11-18 06:38:45,810 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,810 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,810 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rpws21g8
2023-11-18 06:38:45,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fcbc344f-460f-4516-92b0-351ec56839b6
2023-11-18 06:38:45,817 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43067
2023-11-18 06:38:45,819 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43067
2023-11-18 06:38:45,819 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42755
2023-11-18 06:38:45,819 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:45,820 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:45,820 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:45,820 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:45,820 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2wcay0uz
2023-11-18 06:38:45,821 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24777dd5-7c1a-4ca5-bd75-085ac1b4bce6
2023-11-18 06:38:45,821 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57be637b-b031-451b-8aa1-4e71d0ec1bf9
2023-11-18 06:38:46,200 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,211 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2498a2c4-5fc1-4005-b3fa-1fa8879aa9ff
2023-11-18 06:38:46,212 - distributed.worker - INFO - Starting Worker plugin PreImport-7b60b7f2-9e16-4910-a45f-e2ce80b126ba
2023-11-18 06:38:46,212 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,244 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42439', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42439
2023-11-18 06:38:46,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49856
2023-11-18 06:38:46,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,250 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43513', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,251 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43513
2023-11-18 06:38:46,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49862
2023-11-18 06:38:46,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,256 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,257 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,257 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,259 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,259 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d04f095-d4ec-4f50-a0dd-1e540e1a79cc
2023-11-18 06:38:46,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42477', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,288 - distributed.worker - INFO - Starting Worker plugin PreImport-14cd649f-ad08-4ea5-8823-504591ba63f5
2023-11-18 06:38:46,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42477
2023-11-18 06:38:46,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49866
2023-11-18 06:38:46,289 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,294 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,294 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39577', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39577
2023-11-18 06:38:46,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49868
2023-11-18 06:38:46,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,348 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,365 - distributed.worker - INFO - Starting Worker plugin PreImport-2048a41b-e6f9-4ab2-9273-6e640121c5f6
2023-11-18 06:38:46,365 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,371 - distributed.worker - INFO - Starting Worker plugin PreImport-76324463-6cfb-4394-8d3c-e66b0924dbe6
2023-11-18 06:38:46,371 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,376 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fb4c6b4-0c15-4463-b155-68a4f9d8ef5b
2023-11-18 06:38:46,377 - distributed.worker - INFO - Starting Worker plugin PreImport-dcf3a63c-3904-43ac-9316-e47a4d6002fe
2023-11-18 06:38:46,377 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,396 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c606057b-acde-4b82-b728-31f064fe1807
2023-11-18 06:38:46,397 - distributed.worker - INFO - Starting Worker plugin PreImport-41b95613-86ff-4e6f-b2c5-09788ffa0012
2023-11-18 06:38:46,398 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38667', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,401 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38667
2023-11-18 06:38:46,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49872
2023-11-18 06:38:46,402 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,406 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,547 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43833', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43833
2023-11-18 06:38:46,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49902
2023-11-18 06:38:46,549 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42197', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42197
2023-11-18 06:38:46,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49886
2023-11-18 06:38:46,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,552 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43067', status: init, memory: 0, processing: 0>
2023-11-18 06:38:46,556 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43067
2023-11-18 06:38:46,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49884
2023-11-18 06:38:46,558 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:46,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,558 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,559 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,563 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:46,563 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:46,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:46,658 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,658 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,658 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,658 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,659 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,659 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,659 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,659 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,671 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,671 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,671 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,671 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,671 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,671 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,672 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,672 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:38:46,678 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,680 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:38:46,682 - distributed.scheduler - INFO - Remove client Client-1a43e78d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:46,683 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49052; closing.
2023-11-18 06:38:46,683 - distributed.scheduler - INFO - Remove client Client-1a43e78d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:46,683 - distributed.scheduler - INFO - Close client connection: Client-1a43e78d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:46,684 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40913'. Reason: nanny-close
2023-11-18 06:38:46,685 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,686 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37525'. Reason: nanny-close
2023-11-18 06:38:46,686 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,686 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43833. Reason: nanny-close
2023-11-18 06:38:46,686 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45469'. Reason: nanny-close
2023-11-18 06:38:46,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,687 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43067. Reason: nanny-close
2023-11-18 06:38:46,687 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33469'. Reason: nanny-close
2023-11-18 06:38:46,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,687 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43513. Reason: nanny-close
2023-11-18 06:38:46,688 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42373'. Reason: nanny-close
2023-11-18 06:38:46,688 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42477. Reason: nanny-close
2023-11-18 06:38:46,688 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35687'. Reason: nanny-close
2023-11-18 06:38:46,689 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49902; closing.
2023-11-18 06:38:46,689 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,689 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,689 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42439. Reason: nanny-close
2023-11-18 06:38:46,689 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,689 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.6894286')
2023-11-18 06:38:46,689 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45463'. Reason: nanny-close
2023-11-18 06:38:46,689 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,690 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,690 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42197. Reason: nanny-close
2023-11-18 06:38:46,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34425'. Reason: nanny-close
2023-11-18 06:38:46,690 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:46,690 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38667. Reason: nanny-close
2023-11-18 06:38:46,690 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,690 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49862; closing.
2023-11-18 06:38:46,691 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,691 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49884; closing.
2023-11-18 06:38:46,691 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39577. Reason: nanny-close
2023-11-18 06:38:46,691 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,691 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,691 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,692 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,692 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,692 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.692298')
2023-11-18 06:38:46,692 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,692 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.692798')
2023-11-18 06:38:46,693 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,693 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:46,693 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,694 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,695 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:46,694 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49862>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49862>: Stream is closed
2023-11-18 06:38:46,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49866; closing.
2023-11-18 06:38:46,697 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.6970608')
2023-11-18 06:38:46,697 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49856; closing.
2023-11-18 06:38:46,697 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49886; closing.
2023-11-18 06:38:46,697 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49872; closing.
2023-11-18 06:38:46,698 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42439', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.698431')
2023-11-18 06:38:46,698 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.698851')
2023-11-18 06:38:46,699 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.6992226')
2023-11-18 06:38:46,699 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49868; closing.
2023-11-18 06:38:46,700 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39577', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289526.700125')
2023-11-18 06:38:46,700 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:38:46,700 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49868>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-18 06:38:48,452 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:38:48,453 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:38:48,453 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:38:48,454 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:38:48,455 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-11-18 06:38:50,618 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:50,622 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35237 instead
  warnings.warn(
2023-11-18 06:38:50,627 - distributed.scheduler - INFO - State start
2023-11-18 06:38:50,647 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:38:50,648 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:38:50,648 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35237/status
2023-11-18 06:38:50,649 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:38:51,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33057'
2023-11-18 06:38:51,217 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39975'
2023-11-18 06:38:51,233 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38607'
2023-11-18 06:38:51,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32813'
2023-11-18 06:38:51,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42387'
2023-11-18 06:38:51,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34699'
2023-11-18 06:38:51,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40783'
2023-11-18 06:38:51,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38805'
2023-11-18 06:38:51,601 - distributed.scheduler - INFO - Receive client connection: Client-21536a90-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:51,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54596
2023-11-18 06:38:53,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,105 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,143 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:38:53,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:38:53,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:53,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:38:56,347 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39509
2023-11-18 06:38:56,348 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39509
2023-11-18 06:38:56,348 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40153
2023-11-18 06:38:56,348 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,348 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,348 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,348 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,348 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ndlne9c9
2023-11-18 06:38:56,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8588cfe-e0b8-4f0d-8645-85fa97d17d83
2023-11-18 06:38:56,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e35d8c43-e1a0-437e-8d7b-b07829154ccf
2023-11-18 06:38:56,487 - distributed.worker - INFO - Starting Worker plugin PreImport-065224bd-64fa-492e-9a54-3aa1e4d6c30d
2023-11-18 06:38:56,487 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,524 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39509', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,527 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39509
2023-11-18 06:38:56,527 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54616
2023-11-18 06:38:56,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,529 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,530 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,654 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33571
2023-11-18 06:38:56,655 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33571
2023-11-18 06:38:56,655 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45225
2023-11-18 06:38:56,655 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,655 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,654 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32779
2023-11-18 06:38:56,655 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32779
2023-11-18 06:38:56,655 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45197
2023-11-18 06:38:56,655 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,655 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,655 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,655 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,655 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d1ah9qeb
2023-11-18 06:38:56,655 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,656 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,656 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z_ej47n1
2023-11-18 06:38:56,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6ff13ab-a59f-4342-b107-d989631d3ad9
2023-11-18 06:38:56,656 - distributed.worker - INFO - Starting Worker plugin PreImport-92dec3ee-3e02-4f05-b668-c646c8f58799
2023-11-18 06:38:56,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ebb8e9a-1c75-431b-925e-dc03ada586ef
2023-11-18 06:38:56,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0dcf692-ec5c-45a8-a0c9-d26eed8a00bb
2023-11-18 06:38:56,656 - distributed.worker - INFO - Starting Worker plugin PreImport-ea18f2fc-5a5c-4263-8082-6281c91e5b96
2023-11-18 06:38:56,657 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4de51436-8ec6-4a48-8417-39e50743cb18
2023-11-18 06:38:56,657 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35517
2023-11-18 06:38:56,658 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35517
2023-11-18 06:38:56,658 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44439
2023-11-18 06:38:56,658 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,658 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,658 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,658 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,658 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-img8xc6n
2023-11-18 06:38:56,658 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd654338-b6a1-4f29-a1a9-e205c1eeba49
2023-11-18 06:38:56,729 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37437
2023-11-18 06:38:56,729 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37437
2023-11-18 06:38:56,730 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43717
2023-11-18 06:38:56,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,730 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,730 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,730 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1f72uyec
2023-11-18 06:38:56,730 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad2e5196-60f2-42eb-b1e3-cd2e3614e6d3
2023-11-18 06:38:56,736 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36915
2023-11-18 06:38:56,737 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36915
2023-11-18 06:38:56,737 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41039
2023-11-18 06:38:56,737 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,737 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,737 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,738 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7r72fxd6
2023-11-18 06:38:56,737 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35969
2023-11-18 06:38:56,738 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35969
2023-11-18 06:38:56,738 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33095
2023-11-18 06:38:56,738 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,738 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,738 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-315116e8-fcfd-4cc7-be93-03f1afaea4f6
2023-11-18 06:38:56,738 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,738 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z1nmi0_0
2023-11-18 06:38:56,739 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1363bfd-2892-47b1-b7d9-360c81369393
2023-11-18 06:38:56,739 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a0ce0f7-78ea-4963-8570-875b83332b9b
2023-11-18 06:38:56,739 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fe26585-5128-4470-b24a-522bce7e7101
2023-11-18 06:38:56,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40721
2023-11-18 06:38:56,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40721
2023-11-18 06:38:56,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36949
2023-11-18 06:38:56,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,742 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,742 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:38:56,742 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:38:56,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yrje3ows
2023-11-18 06:38:56,742 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4dc009f6-36fb-4ba9-9430-bf5df5f8e617
2023-11-18 06:38:56,856 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07450e15-0655-48e2-ac0c-9859e03eab02
2023-11-18 06:38:56,856 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,856 - distributed.worker - INFO - Starting Worker plugin PreImport-e4d662eb-bdd5-49d4-baeb-772cf6174f8f
2023-11-18 06:38:56,856 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,864 - distributed.worker - INFO - Starting Worker plugin PreImport-a361f1ce-1df2-4c26-add8-6d59cf4ae808
2023-11-18 06:38:56,865 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81a32513-8c8f-4867-9404-e47b67f994cb
2023-11-18 06:38:56,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0bc1846-443f-4e14-8e48-26b98e92b5cc
2023-11-18 06:38:56,879 - distributed.worker - INFO - Starting Worker plugin PreImport-91de3351-07c9-4fd7-863a-fa7396b9630f
2023-11-18 06:38:56,879 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,879 - distributed.worker - INFO - Starting Worker plugin PreImport-de5cce5f-3fc1-4beb-8801-48d7b4878074
2023-11-18 06:38:56,880 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32779', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,889 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32779
2023-11-18 06:38:56,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54622
2023-11-18 06:38:56,890 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35517', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35517
2023-11-18 06:38:56,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54632
2023-11-18 06:38:56,891 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,891 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,892 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,892 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35969', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35969
2023-11-18 06:38:56,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54642
2023-11-18 06:38:56,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,896 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,905 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40721', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,906 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40721
2023-11-18 06:38:56,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54644
2023-11-18 06:38:56,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,907 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,907 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,914 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,920 - distributed.worker - INFO - Starting Worker plugin PreImport-9ba6e7de-8874-476b-a419-638fca09f9b6
2023-11-18 06:38:56,921 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37437', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37437
2023-11-18 06:38:56,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54646
2023-11-18 06:38:56,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,926 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33571', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,954 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33571
2023-11-18 06:38:56,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54656
2023-11-18 06:38:56,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,957 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,957 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,958 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36915', status: init, memory: 0, processing: 0>
2023-11-18 06:38:56,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36915
2023-11-18 06:38:56,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54658
2023-11-18 06:38:56,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:38:56,961 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:38:56,961 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:38:56,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:56,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:38:57,017 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,017 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,019 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:38:57,023 - distributed.scheduler - INFO - Remove client Client-21536a90-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:57,023 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54596; closing.
2023-11-18 06:38:57,023 - distributed.scheduler - INFO - Remove client Client-21536a90-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:57,023 - distributed.scheduler - INFO - Close client connection: Client-21536a90-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:38:57,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33057'. Reason: nanny-close
2023-11-18 06:38:57,025 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39975'. Reason: nanny-close
2023-11-18 06:38:57,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,027 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35517. Reason: nanny-close
2023-11-18 06:38:57,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38607'. Reason: nanny-close
2023-11-18 06:38:57,027 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,027 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35969. Reason: nanny-close
2023-11-18 06:38:57,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32813'. Reason: nanny-close
2023-11-18 06:38:57,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,028 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39509. Reason: nanny-close
2023-11-18 06:38:57,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42387'. Reason: nanny-close
2023-11-18 06:38:57,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,028 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,028 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54632; closing.
2023-11-18 06:38:57,028 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33571. Reason: nanny-close
2023-11-18 06:38:57,029 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0291505')
2023-11-18 06:38:57,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34699'. Reason: nanny-close
2023-11-18 06:38:57,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,029 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32779. Reason: nanny-close
2023-11-18 06:38:57,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40783'. Reason: nanny-close
2023-11-18 06:38:57,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40721. Reason: nanny-close
2023-11-18 06:38:57,030 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38805'. Reason: nanny-close
2023-11-18 06:38:57,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:38:57,030 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,030 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54642; closing.
2023-11-18 06:38:57,030 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,031 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0317712')
2023-11-18 06:38:57,031 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37437. Reason: nanny-close
2023-11-18 06:38:57,032 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36915. Reason: nanny-close
2023-11-18 06:38:57,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54616; closing.
2023-11-18 06:38:57,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39509', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0327394')
2023-11-18 06:38:57,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54656; closing.
2023-11-18 06:38:57,033 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,033 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,033 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33571', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0337753')
2023-11-18 06:38:57,033 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54622; closing.
2023-11-18 06:38:57,034 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54644; closing.
2023-11-18 06:38:57,034 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32779', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0348577')
2023-11-18 06:38:57,034 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:38:57,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0352354')
2023-11-18 06:38:57,035 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54646; closing.
2023-11-18 06:38:57,035 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:57,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37437', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.0360548')
2023-11-18 06:38:57,036 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54658; closing.
2023-11-18 06:38:57,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36915', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289537.036768')
2023-11-18 06:38:57,037 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:38:57,037 - distributed.nanny - INFO - Worker closed
2023-11-18 06:38:58,793 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:38:58,793 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:38:58,794 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:38:58,796 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:38:58,796 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-11-18 06:39:01,040 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:01,044 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43897 instead
  warnings.warn(
2023-11-18 06:39:01,048 - distributed.scheduler - INFO - State start
2023-11-18 06:39:01,313 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:01,314 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:39:01,315 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43897/status
2023-11-18 06:39:01,315 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:01,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35457'
2023-11-18 06:39:01,512 - distributed.scheduler - INFO - Receive client connection: Client-27994f52-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:01,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41148
2023-11-18 06:39:03,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:03,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:04,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:05,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41537
2023-11-18 06:39:05,797 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41537
2023-11-18 06:39:05,797 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-11-18 06:39:05,797 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:05,797 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:05,797 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:05,797 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-18 06:39:05,797 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xm3y954v
2023-11-18 06:39:05,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72854222-fdde-46df-93c5-c1057a1eabd5
2023-11-18 06:39:05,798 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0f45b19-921d-4cd4-882e-80fb081e82b7
2023-11-18 06:39:05,798 - distributed.worker - INFO - Starting Worker plugin PreImport-15947c6c-1c79-4cd8-a535-04feee7d2a84
2023-11-18 06:39:05,799 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:05,844 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41537', status: init, memory: 0, processing: 0>
2023-11-18 06:39:05,845 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41537
2023-11-18 06:39:05,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41168
2023-11-18 06:39:05,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:05,847 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:05,847 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:05,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:05,859 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:05,862 - distributed.scheduler - INFO - Remove client Client-27994f52-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:05,862 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41148; closing.
2023-11-18 06:39:05,862 - distributed.scheduler - INFO - Remove client Client-27994f52-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:05,863 - distributed.scheduler - INFO - Close client connection: Client-27994f52-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:05,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35457'. Reason: nanny-close
2023-11-18 06:39:05,883 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:05,885 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41537. Reason: nanny-close
2023-11-18 06:39:05,887 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:05,887 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41168; closing.
2023-11-18 06:39:05,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289545.887894')
2023-11-18 06:39:05,888 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:39:05,889 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:06,830 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:06,830 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:06,831 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:06,832 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:39:06,832 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-11-18 06:39:10,797 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:10,802 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44043 instead
  warnings.warn(
2023-11-18 06:39:10,806 - distributed.scheduler - INFO - State start
2023-11-18 06:39:10,829 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:10,830 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:39:10,831 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44043/status
2023-11-18 06:39:10,831 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:10,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46791'
2023-11-18 06:39:12,169 - distributed.scheduler - INFO - Receive client connection: Client-2d5c185d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:12,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38730
2023-11-18 06:39:12,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:12,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:13,461 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:17,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37047
2023-11-18 06:39:17,666 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37047
2023-11-18 06:39:17,666 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41689
2023-11-18 06:39:17,666 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:17,666 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:17,666 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:17,666 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-18 06:39:17,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-757jk8ja
2023-11-18 06:39:17,668 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df0e1040-518d-44fd-8a6e-d1566a3eec8c
2023-11-18 06:39:17,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-203ed986-5e7c-47ac-b3c1-c0a6b7980b6f
2023-11-18 06:39:17,668 - distributed.worker - INFO - Starting Worker plugin PreImport-a1038718-e8ee-4d3a-82bc-1d01483b137c
2023-11-18 06:39:17,670 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:17,717 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37047', status: init, memory: 0, processing: 0>
2023-11-18 06:39:17,719 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37047
2023-11-18 06:39:17,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38748
2023-11-18 06:39:17,720 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:17,721 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:17,722 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:17,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:17,738 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:17,741 - distributed.scheduler - INFO - Remove client Client-2d5c185d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:17,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38730; closing.
2023-11-18 06:39:17,742 - distributed.scheduler - INFO - Remove client Client-2d5c185d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:17,742 - distributed.scheduler - INFO - Close client connection: Client-2d5c185d-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:17,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46791'. Reason: nanny-close
2023-11-18 06:39:17,743 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:17,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37047. Reason: nanny-close
2023-11-18 06:39:17,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38748; closing.
2023-11-18 06:39:17,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:17,747 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289557.74744')
2023-11-18 06:39:17,747 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:39:17,749 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:18,910 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:18,911 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:18,912 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:18,913 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:39:18,914 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-11-18 06:39:21,296 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:21,300 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33307 instead
  warnings.warn(
2023-11-18 06:39:21,304 - distributed.scheduler - INFO - State start
2023-11-18 06:39:21,325 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:21,326 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:39:21,327 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33307/status
2023-11-18 06:39:21,327 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:25,789 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:48800'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48800>: Stream is closed
2023-11-18 06:39:26,063 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:26,064 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:26,064 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:26,065 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:39:26,065 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-11-18 06:39:28,279 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:28,284 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46217 instead
  warnings.warn(
2023-11-18 06:39:28,288 - distributed.scheduler - INFO - State start
2023-11-18 06:39:28,310 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:28,311 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-18 06:39:28,312 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46217/status
2023-11-18 06:39:28,312 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:28,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34751'
2023-11-18 06:39:28,983 - distributed.scheduler - INFO - Receive client connection: Client-37c08b33-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:28,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55322
2023-11-18 06:39:30,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:30,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:30,114 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:31,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44167
2023-11-18 06:39:31,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44167
2023-11-18 06:39:31,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42829
2023-11-18 06:39:31,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-18 06:39:31,042 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:31,042 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:31,042 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-18 06:39:31,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-r2seg7jv
2023-11-18 06:39:31,042 - distributed.worker - INFO - Starting Worker plugin PreImport-b685f71e-7558-4e48-8658-84bba6dc285f
2023-11-18 06:39:31,043 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bcfd667e-7035-46be-8a33-6ba37542bf9b
2023-11-18 06:39:31,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3072a92-f03a-4476-bc0a-eae676510475
2023-11-18 06:39:31,043 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:31,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44167', status: init, memory: 0, processing: 0>
2023-11-18 06:39:31,083 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44167
2023-11-18 06:39:31,083 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49912
2023-11-18 06:39:31,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:31,085 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-18 06:39:31,085 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:31,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-18 06:39:31,161 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:31,164 - distributed.scheduler - INFO - Remove client Client-37c08b33-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:31,164 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55322; closing.
2023-11-18 06:39:31,164 - distributed.scheduler - INFO - Remove client Client-37c08b33-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:31,165 - distributed.scheduler - INFO - Close client connection: Client-37c08b33-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:31,166 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34751'. Reason: nanny-close
2023-11-18 06:39:31,166 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:31,167 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44167. Reason: nanny-close
2023-11-18 06:39:31,170 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49912; closing.
2023-11-18 06:39:31,170 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-18 06:39:31,170 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44167', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289571.1704264')
2023-11-18 06:39:31,170 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:39:31,172 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:33,033 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:33,034 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:33,035 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:33,036 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-18 06:39:33,037 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-11-18 06:39:35,257 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:35,261 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46255 instead
  warnings.warn(
2023-11-18 06:39:35,265 - distributed.scheduler - INFO - State start
2023-11-18 06:39:35,288 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:35,289 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:39:35,290 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46255/status
2023-11-18 06:39:35,291 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:35,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36201'
2023-11-18 06:39:35,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33119'
2023-11-18 06:39:35,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46035'
2023-11-18 06:39:35,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44831'
2023-11-18 06:39:35,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40893'
2023-11-18 06:39:35,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40697'
2023-11-18 06:39:35,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40761'
2023-11-18 06:39:35,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44609'
2023-11-18 06:39:36,862 - distributed.scheduler - INFO - Receive client connection: Client-3bf3a458-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:36,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55134
2023-11-18 06:39:37,526 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,526 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,530 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,566 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,566 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,570 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,622 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:37,636 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:37,636 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:37,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:42,383 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37277
2023-11-18 06:39:42,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37277
2023-11-18 06:39:42,384 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35789
2023-11-18 06:39:42,384 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,384 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,384 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,384 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,384 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w3e406w0
2023-11-18 06:39:42,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c88aa8d-500c-4461-a0b5-59e27625a58b
2023-11-18 06:39:42,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44191
2023-11-18 06:39:42,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44191
2023-11-18 06:39:42,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44635
2023-11-18 06:39:42,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,392 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,392 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j9_4p5gj
2023-11-18 06:39:42,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3190da68-5d4d-4711-b6bd-777e670e7462
2023-11-18 06:39:42,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65e964f2-983f-4f9d-990b-28ba4dc79526
2023-11-18 06:39:42,504 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37831
2023-11-18 06:39:42,505 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37831
2023-11-18 06:39:42,505 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38613
2023-11-18 06:39:42,505 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,505 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,505 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,505 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,505 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yix3dhit
2023-11-18 06:39:42,506 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bdf502da-9214-4383-b015-e0d426f3e369
2023-11-18 06:39:42,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35019
2023-11-18 06:39:42,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35019
2023-11-18 06:39:42,508 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43189
2023-11-18 06:39:42,508 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,508 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,509 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i_ocs8kn
2023-11-18 06:39:42,509 - distributed.worker - INFO - Starting Worker plugin PreImport-3ce7cf95-9df8-45d6-aac8-70e7b1c6664c
2023-11-18 06:39:42,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f3ffe4a-3fac-4cfc-8ee4-0e6175064aa1
2023-11-18 06:39:42,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4129bd8-e479-4cd5-b24a-d54e35130330
2023-11-18 06:39:42,510 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38325
2023-11-18 06:39:42,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38325
2023-11-18 06:39:42,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46037
2023-11-18 06:39:42,511 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,511 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,510 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43111
2023-11-18 06:39:42,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,509 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42523
2023-11-18 06:39:42,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q6xi9wap
2023-11-18 06:39:42,511 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43111
2023-11-18 06:39:42,511 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42523
2023-11-18 06:39:42,511 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38721
2023-11-18 06:39:42,511 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,511 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35841
2023-11-18 06:39:42,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,511 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,511 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f79fcd8e-cf67-4d49-91b2-c4c726ff29c5
2023-11-18 06:39:42,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ykutvt8
2023-11-18 06:39:42,511 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,512 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zvpmtgp0
2023-11-18 06:39:42,512 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8fdf166e-b0cb-42c6-942d-16222c1c604a
2023-11-18 06:39:42,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d197b3be-5439-47d0-b2ad-98be7cc21a84
2023-11-18 06:39:42,513 - distributed.worker - INFO - Starting Worker plugin PreImport-ab859c9f-e0fa-427e-b173-15e74e9837b5
2023-11-18 06:39:42,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1249f8fc-2c35-405b-9376-b6cd58b975ce
2023-11-18 06:39:42,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34069
2023-11-18 06:39:42,514 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34069
2023-11-18 06:39:42,514 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35673
2023-11-18 06:39:42,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,514 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,514 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:42,514 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-18 06:39:42,514 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a3hc61hp
2023-11-18 06:39:42,515 - distributed.worker - INFO - Starting Worker plugin PreImport-99fb5f06-b9a5-4e84-9742-e2240d291af1
2023-11-18 06:39:42,515 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b2f6e8f-f437-4bd5-b713-d90baf85c0e2
2023-11-18 06:39:42,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3a0174c-74ae-4bf2-9db9-b0f9be97e3ac
2023-11-18 06:39:42,696 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ddb5ebad-3649-432c-b3e9-56b5923434a1
2023-11-18 06:39:42,696 - distributed.worker - INFO - Starting Worker plugin PreImport-876f0a9d-ee8a-4164-a0d5-1f4c60462472
2023-11-18 06:39:42,697 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,697 - distributed.worker - INFO - Starting Worker plugin PreImport-9dd3ed66-9ee2-4f76-ac59-60e656cc3bef
2023-11-18 06:39:42,698 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-542449a9-bebb-4eef-add8-ac203ba6c71c
2023-11-18 06:39:42,699 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,700 - distributed.worker - INFO - Starting Worker plugin PreImport-0c6a7842-5ec9-4bea-8c4e-a2580a720920
2023-11-18 06:39:42,700 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,700 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,708 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5ef3739-32ca-4401-8741-34600473728c
2023-11-18 06:39:42,709 - distributed.worker - INFO - Starting Worker plugin PreImport-c936f4a5-baa3-40a1-a5fb-b2275bcb07be
2023-11-18 06:39:42,709 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99bb3dad-3f8b-461c-97a2-2f43de4d0ed0
2023-11-18 06:39:42,712 - distributed.worker - INFO - Starting Worker plugin PreImport-9fbbb374-a6f2-46fa-8ceb-983b4cb5fe3e
2023-11-18 06:39:42,712 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,728 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37277', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,730 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37277
2023-11-18 06:39:42,730 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58610
2023-11-18 06:39:42,731 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35019', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,731 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,731 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35019
2023-11-18 06:39:42,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58620
2023-11-18 06:39:42,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,735 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,735 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,736 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,736 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42523', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,737 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42523
2023-11-18 06:39:42,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58644
2023-11-18 06:39:42,738 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34069', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,739 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34069
2023-11-18 06:39:42,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58664
2023-11-18 06:39:42,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,740 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38325', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,740 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38325
2023-11-18 06:39:42,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58654
2023-11-18 06:39:42,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43111', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43111
2023-11-18 06:39:42,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58676
2023-11-18 06:39:42,743 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44191', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,743 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,744 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44191
2023-11-18 06:39:42,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58628
2023-11-18 06:39:42,744 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,744 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,744 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,744 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,748 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,748 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,751 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,751 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,756 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,761 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37831', status: init, memory: 0, processing: 0>
2023-11-18 06:39:42,762 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37831
2023-11-18 06:39:42,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58690
2023-11-18 06:39:42,764 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:42,773 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:42,773 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:42,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:42,788 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,788 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,788 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:42,805 - distributed.scheduler - INFO - Remove client Client-3bf3a458-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:42,805 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55134; closing.
2023-11-18 06:39:42,805 - distributed.scheduler - INFO - Remove client Client-3bf3a458-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:42,806 - distributed.scheduler - INFO - Close client connection: Client-3bf3a458-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:42,807 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36201'. Reason: nanny-close
2023-11-18 06:39:42,807 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,808 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33119'. Reason: nanny-close
2023-11-18 06:39:42,808 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,808 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34069. Reason: nanny-close
2023-11-18 06:39:42,809 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46035'. Reason: nanny-close
2023-11-18 06:39:42,809 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,809 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43111. Reason: nanny-close
2023-11-18 06:39:42,809 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44831'. Reason: nanny-close
2023-11-18 06:39:42,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,810 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44191. Reason: nanny-close
2023-11-18 06:39:42,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40893'. Reason: nanny-close
2023-11-18 06:39:42,810 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,810 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58664; closing.
2023-11-18 06:39:42,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38325. Reason: nanny-close
2023-11-18 06:39:42,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40697'. Reason: nanny-close
2023-11-18 06:39:42,811 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34069', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.811272')
2023-11-18 06:39:42,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,811 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37277. Reason: nanny-close
2023-11-18 06:39:42,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40761'. Reason: nanny-close
2023-11-18 06:39:42,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35019. Reason: nanny-close
2023-11-18 06:39:42,812 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44609'. Reason: nanny-close
2023-11-18 06:39:42,812 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,812 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58676; closing.
2023-11-18 06:39:42,812 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42523. Reason: nanny-close
2023-11-18 06:39:42,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8137002')
2023-11-18 06:39:42,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58628; closing.
2023-11-18 06:39:42,814 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,814 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,814 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8145757')
2023-11-18 06:39:42,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58654; closing.
2023-11-18 06:39:42,815 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,815 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,815 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38325', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8156235')
2023-11-18 06:39:42,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,815 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58610; closing.
2023-11-18 06:39:42,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58620; closing.
2023-11-18 06:39:42,816 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37277', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8166478')
2023-11-18 06:39:42,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8171217')
2023-11-18 06:39:42,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58644; closing.
2023-11-18 06:39:42,817 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:42,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8178113')
2023-11-18 06:39:42,825 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:42,826 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37831. Reason: nanny-close
2023-11-18 06:39:42,828 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58690; closing.
2023-11-18 06:39:42,828 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:42,829 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289582.8290265')
2023-11-18 06:39:42,829 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:39:42,830 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:44,575 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:44,576 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:44,576 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:44,578 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:39:44,578 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-11-18 06:39:46,660 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:46,664 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45267 instead
  warnings.warn(
2023-11-18 06:39:46,667 - distributed.scheduler - INFO - State start
2023-11-18 06:39:46,687 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:46,688 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:39:46,688 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45267/status
2023-11-18 06:39:46,688 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:46,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44647'
2023-11-18 06:39:47,617 - distributed.scheduler - INFO - Receive client connection: Client-42c74a26-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:47,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58778
2023-11-18 06:39:48,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:48,708 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:48,712 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:50,631 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40269
2023-11-18 06:39:50,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40269
2023-11-18 06:39:50,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45483
2023-11-18 06:39:50,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:50,634 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:50,634 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:50,634 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-18 06:39:50,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-os1f9fgw
2023-11-18 06:39:50,636 - distributed.worker - INFO - Starting Worker plugin PreImport-5c56d8bf-27bf-4e32-986f-caa6c012994e
2023-11-18 06:39:50,636 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0b6f6ed-dd0f-45d2-8c99-8c1ff3f6f51d
2023-11-18 06:39:50,780 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b89c8409-46d2-4a69-9237-d4ff4950d64a
2023-11-18 06:39:50,780 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:50,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40269', status: init, memory: 0, processing: 0>
2023-11-18 06:39:50,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40269
2023-11-18 06:39:50,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36646
2023-11-18 06:39:50,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:50,816 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:50,816 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:50,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:50,895 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:39:50,899 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:50,900 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:50,903 - distributed.scheduler - INFO - Remove client Client-42c74a26-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:50,903 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58778; closing.
2023-11-18 06:39:50,903 - distributed.scheduler - INFO - Remove client Client-42c74a26-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:50,903 - distributed.scheduler - INFO - Close client connection: Client-42c74a26-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:50,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44647'. Reason: nanny-close
2023-11-18 06:39:50,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:50,906 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40269. Reason: nanny-close
2023-11-18 06:39:50,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36646; closing.
2023-11-18 06:39:50,908 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:50,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289590.9085393')
2023-11-18 06:39:50,908 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:39:50,910 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:52,572 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:52,573 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:52,573 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:52,574 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:39:52,575 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-11-18 06:39:54,829 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:54,833 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38815 instead
  warnings.warn(
2023-11-18 06:39:54,836 - distributed.scheduler - INFO - State start
2023-11-18 06:39:54,857 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-18 06:39:54,858 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-18 06:39:54,859 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38815/status
2023-11-18 06:39:54,859 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-18 06:39:54,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40033'
2023-11-18 06:39:55,385 - distributed.scheduler - INFO - Receive client connection: Client-479f7078-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:55,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36734
2023-11-18 06:39:56,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-18 06:39:56,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-18 06:39:56,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-18 06:39:57,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40375
2023-11-18 06:39:57,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40375
2023-11-18 06:39:57,545 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38795
2023-11-18 06:39:57,545 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-18 06:39:57,546 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:57,546 - distributed.worker - INFO -               Threads:                          1
2023-11-18 06:39:57,546 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-18 06:39:57,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kyjayc99
2023-11-18 06:39:57,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb1b78f2-cd44-486d-bcba-043c11a3ebe9
2023-11-18 06:39:57,655 - distributed.worker - INFO - Starting Worker plugin PreImport-5b931d0d-d597-44ef-a61e-031d6a3b42d7
2023-11-18 06:39:57,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b936666-aae8-4479-94c2-cc5a91b7a769
2023-11-18 06:39:57,655 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:57,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40375', status: init, memory: 0, processing: 0>
2023-11-18 06:39:57,684 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40375
2023-11-18 06:39:57,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36754
2023-11-18 06:39:57,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-18 06:39:57,686 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-18 06:39:57,686 - distributed.worker - INFO - -------------------------------------------------
2023-11-18 06:39:57,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-18 06:39:57,744 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-11-18 06:39:57,748 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-18 06:39:57,751 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:57,752 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-18 06:39:57,755 - distributed.scheduler - INFO - Remove client Client-479f7078-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:57,755 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36734; closing.
2023-11-18 06:39:57,755 - distributed.scheduler - INFO - Remove client Client-479f7078-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:57,756 - distributed.scheduler - INFO - Close client connection: Client-479f7078-85dd-11ee-b255-d8c49764f6bb
2023-11-18 06:39:57,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40033'. Reason: nanny-close
2023-11-18 06:39:57,757 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-18 06:39:57,758 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40375. Reason: nanny-close
2023-11-18 06:39:57,760 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36754; closing.
2023-11-18 06:39:57,760 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-18 06:39:57,760 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40375', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700289597.7609062')
2023-11-18 06:39:57,761 - distributed.scheduler - INFO - Lost all workers
2023-11-18 06:39:57,762 - distributed.nanny - INFO - Worker closed
2023-11-18 06:39:58,873 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-18 06:39:58,873 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-18 06:39:58,874 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-18 06:39:58,875 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-18 06:39:58,875 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33593 instead
  warnings.warn(
2023-11-18 06:40:25,279 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-11-18 06:40:25,283 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:48193', name: 6, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46339 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33197 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46681 instead
  warnings.warn(
2023-11-18 06:41:10,749 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-11-18 06:41:10,749 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-11-18 06:41:10,753 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1458' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1459' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1467' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40395 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45549 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46137 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44807 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39549 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40273 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42135 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42895 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45501 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43125 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38701 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42917 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34543 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33151 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33985 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42327 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32783 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34029 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] 2023-11-18 06:47:33,167 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 81, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 507, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 434, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-11-18 06:47:33,403 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-d0459673-edc1-445f-8935-59cef9c8ad49
Function:  _run_coroutine_on_worker
args:      (24815026835395522555473446330369233516, <function shuffle_task at 0x7f7084286550>, ('explicit-comms-shuffle-e097386536174eabd41defafccacc725', {0: set(), 1: set(), 2: {('from_pandas-042f56ad2fd1cc4d095a550ce07a5908', 0)}}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 379, in shuffle_task
    await send_recv_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 296, in send_recv_partitions
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 124, in recv
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 120, in read_msg
    msg: Dict[int, DataFrame] = nested_deserialize(await eps[rank].read())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
    ret = unpack_construct(&ctx, buf, buf_len, &off)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 81, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 507, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 434, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37065 instead
  warnings.warn(
2023-11-18 06:48:33,280 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-18 06:48:33,486 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-18 06:48:33,513 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:43911'.
2023-11-18 06:48:33,514 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:43911'. Shutting down.
2023-11-18 06:48:33,540 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f36ed810c70>>, <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-18 06:48:35,543 - distributed.nanny - ERROR - Worker process died unexpectedly
